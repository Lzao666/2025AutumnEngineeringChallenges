# Level0: Linux、Docker、Git 与 vLLM 的学习笔记

## 1. Linux

**常用的 Linux 命令行指令：**

*   **文件与目录操作：**
    *   `pwd`：显示当前工作目录的路径。
    *   `ls`：列出目录内容。常用 `-l`（详细信息）、`-a`（显示隐藏文件）。
    *   `cd`：切换目录。如 `cd /home`, `cd ..`（返回上一级）。
    *   `mkdir`：创建新目录。
    *   `rm`：删除文件或目录。`-r`（递归删除目录），`-f`（强制删除，慎用）。
    *   `cp`：复制文件或目录。`-r`（复制目录）。
    *   `mv`：移动文件或目录，也可用于重命名。
    *   `touch`：创建空文件或更新文件时间戳。
*   **文件查看与编辑：**
    *   `cat`：连接文件并打印到标准输出设备（查看文件全部内容）。
    *   `nano` / `vim`：文本编辑器。`nano` 简单易用，`vim` 功能强大但学习曲线稍陡。
*   **系统与进程管理：**
    *   `ps`：显示当前进程状态。常用 `ps aux` 或 `ps -ef`。
    *   `top` / `htop`：动态实时显示进程状态和系统资源占用情况。
    *   `kill`：终止进程。常用 `kill -9 <PID>`（强制终止）。
*   **权限管理：**
    *   `chmod`：改变文件或目录的权限（读 r=4, 写 w=2, 执行 x=1）。如 `chmod 755 114514.sh`。
    *   `chown`：改变文件或目录的所有者和所属组。
*   **网络操作：**
    *   `ping`：测试网络连通性。
    *   `curl` / `wget`：命令行下载工具，常用于测试 API 或下载文件。
*   **压缩与解压：**
    *   `tar`：打包和解包文件。常用 `-czvf`（创建 .tar.gz）、`-xzvf`（解压 .tar.gz）。
*   **查找：**
    *   `find`：在目录树中查找文件。如 `find /home -name "*.log"`。
    *   `grep`：在文件中搜索指定的字符串模式。如 `ps aux | grep python`。

## 2. Docker

**容器 (Container)**
容器是一种轻量级、可移植的软件打包技术，它将应用程序及其所有依赖项（库、配置文件、环境变量等）打包在一个标准化单元中。容器在操作系统级别实现虚拟化，共享主机系统的内核，但在用户空间相互隔离。

**镜像 (Image)**
镜像是容器的模板，是一个只读的文件系统快照。它包含了运行应用程序所需的一切：代码、运行时、库、环境变量和配置文件。镜像通过分层构建（Layer），每一层代表 Dockerfile 中的一条指令，这种结构使得镜像可以高效地共享和存储。

**Dockerfile**
Dockerfile 是一个文本文件，包含了一系列用于构建 Docker 镜像的指令（如 `FROM`, `RUN`, `COPY`, `CMD` 等）。通过执行 `docker build` 命令，Docker 引擎会读取 Dockerfile 中的指令，逐步构建出最终的镜像。

**Docker 与传统虚拟机 (VM) 相比的优势**

|      特性      |             Docker 容器            | 传统虚拟机 (VM) |
|     :--:      |            :--:                    | :--: |
|  **虚拟化级别** |       操作系统级（共享主机内核）       | 硬件级（通过 Hypervisor 虚拟硬件） |
|   **性能**     | **更高**，几乎没有额外开销，接近原生速度 | **较低**，存在 Hypervisor 转换和虚拟硬件开销 |
|   **启动速度**  |       **极快**（秒级甚至毫秒级）      | **较慢**（分钟级） |
|   **磁盘占用**  |  **小得多**（MB 级别），镜像可分层共享  | **非常大**（GB 级别），每个 VM 包含完整 OS |
|    **隔离性**   |       进程级别隔离，**较弱**         | 完整的系统级别隔离，**更强** |
|  **可移植性**   | 强，镜像可在任何支持 Docker 的平台运行 | 较强，但镜像与 Hypervisor 类型绑定 |

**核心优势总结：** Docker 通过**共享内核**和**分层镜像**机制，实现了极高的资源利用率和部署效率，特别适合微服务架构和持续集成/持续部署 (CI/CD) 流程。


## 3. Git

Git 是一个分布式版本控制系统，用于跟踪文件 changes，协调多人协作开发，并管理项目的历史版本。

**基本工作流程：**
1.  **工作区 (Working Directory)**：本地看到的实际文件。
2.  **暂存区 (Staging Area)**：使用 `git add` 将文件的修改添加到暂存区，准备下次提交。
3.  **本地仓库 (Local Repository)**：使用 `git commit` 将暂存区的内容提交到本地仓库，形成一个永久的快照。
4.  **远程仓库 (Remote Repository)**：使用 `git push` 将本地仓库的提交推送到服务器上的远程仓库（如 GitHub, GitLab）进行共享和备份。使用 `git pull` 或 `git fetch` + `git merge` 从远程仓库获取他人的更新并合并到本地。

**关键命令：**

*   `git clone <url>`   
> **作用**：从远程仓库**克隆**一个完整的项目副本到本地。这是获取一个已有项目代码的初始操作。    
*   `git pull`  
> **作用**：从远程仓库**拉取**更新并**立即合并**到当前本地分支。它相当于执行了 `git fetch`（获取更新）和 `git merge`（合并更新）两个操作。用于获取团队其他成员的最新代码。  
*   `git push`  
> **作用**：将本地仓库的提交**推送**到远程仓库。用于分享你的代码更改给团队其他人。  


## 4. vLLM

vLLM (Vectorized Large Language Model serving engine) 是一个专为**高效服务大语言模型 (LLM)** 而设计的高吞吐量、低延迟的推理和服务引擎。

**它为何能让大模型推理更快、更高效？**
vLLM 的核心优化在于其创新的**内存管理机制**和**调度算法**，而非模型计算本身。
1.  **PagedAttention 算法**：这是 vLLM 性能提升的关键。它借鉴了操作系统中虚拟内存和分页的思想，解决了 LLM 推理中 KV Cache 内存管理的瓶颈。
2.  **Continuous Batching**：也称为迭代级调度（Iteration-level Scheduling）或动态批处理。传统的批处理需要等一个批次的所有请求都生成完毕后才处理下一个批次，效率低下。vLLM 实现了**异步处理**，每当某个请求生成完一个 token，它就立即调度下一个请求的计算，极大提高了 GPU 的利用率，尤其是在流式输出场景下。
3.  **优化了的内核**：vLLM 实现了高度优化的 CUDA 内核，高效地执行注意力计算等操作。

**PagedAttention 是什么原理？**
1.  **问题**：在自回归生成过程中，模型需要缓存每个先前生成 token 的键和值（KV Cache），以供后续生成时计算注意力之用。这个 KV Cache 非常大，而且是**动态增长**的。传统方式为每个请求预先分配一个最大可能长度的连续内存块，这会导致**严重的内存内部碎片化**（Internal Fragmentation），因为大多数请求的实际生成长度远小于最大值，大量内存被预留但未被使用。
2.  **原理 - 分页 (Paging)**：
    *   vLLM 将每个请求的 KV Cache 在物理内存上划分为多个**固定大小的块（Page）**，类似于操作系统的内存页。
    *   这些块**不需要连续**存储。
    *   系统维护一个**逻辑上的“块表”**（Block Table），来记录每个请求的序列 tokens 具体存储在哪些物理块中。
3.  **优势**：
    *   **消除内部碎片**：固定大小的块几乎可以被完全利用，极大减少了内存浪费。
    *   **高效内存共享**：在并行采样（如 beam search）等场景中，不同序列可能共享前缀。PagedAttention 允许这些序列**物理上共享**存储前缀 tokens 的块，只需在逻辑上引用即可，避免了重复存储，节省了大量内存。
    *   **效果**：更高效的内存使用意味着可以在**相同的 GPU 内存下运行更大的批次（Batch Size）或更大的模型**，从而显著提升**吞吐量（Throughput）**。
    

## 5. 熟悉 Hunyuan-MT-7B

**Hunyuan-MT-7B** 是腾讯推出的一款专注于**翻译任务**的大语言模型。
*   **参数规模**：70亿（7B）参数。
*   **核心任务**：多语言机器翻译。它旨在理解和生成高质量、流畅的翻译文本。
*   **支持语种**：根据官方信息，它支持**多种主流语言**与**中文**之间的互译。
*   **特点**：作为专注于翻译的模型，它在翻译质量、上下文理解和术语一致性方面相比通用 LLM 进行了专项优化。
